import os
import sys
import logging
import asyncio
import time
import functools
import hashlib
from typing import Dict, List, Any, Optional, Union

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate
from langchain_community.callbacks import get_openai_callback

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„æ·»åŠ åˆ° sys.path
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, PROJECT_ROOT)

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, '.env'))

# è¨­ç½®æ—¥èªŒ
LOGS_DIR = os.path.join(PROJECT_ROOT, 'logs')
os.makedirs(LOGS_DIR, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(LOGS_DIR, "qa_client.log")),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–OpenAI APIå¯†é‘°
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
if OPENAI_API_KEY:
    os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
    logger.info(f"OPENAI_API_KEY å·²è¨­ç½®: {OPENAI_API_KEY[:5]}...")
else:
    logger.warning("OPENAI_API_KEY æœªè¨­ç½®")

# ç³»çµ±æç¤º
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è²¡å‹™ç¨…æ³•é¡§å•ï¼Œè² è²¬å›ç­”ç”¨æˆ¶çš„è²¡å‹™å’Œç¨…æ³•å•é¡Œã€‚

è«‹éµå¾ªä»¥ä¸‹æŒ‡å°åŸå‰‡ï¼š
1. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ‰€æœ‰å•é¡Œï¼Œå³ä½¿ç”¨æˆ¶ä½¿ç”¨ç°¡é«”ä¸­æ–‡æå•ã€‚
2. é¦–å…ˆä»”ç´°åˆ†æç”¨æˆ¶å•é¡Œçš„çœŸæ­£æ„åœ–å’Œèªç¾©ï¼Œç†è§£ç”¨æˆ¶çœŸæ­£æƒ³çŸ¥é“çš„æ˜¯ä»€éº¼ã€‚
3. åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼Œä½†ä¸è¦åƒ…åƒ…è¤‡è£½æ–‡æª”ä¸­çš„å…§å®¹ã€‚
4. å¦‚æœæ–‡æª”ä¸­çš„ä¿¡æ¯ä¸å®Œæ•´ï¼Œè«‹ä½¿ç”¨ä½ çš„å°ˆæ¥­çŸ¥è­˜è£œå……å›ç­”ï¼Œä½†æ˜ç¢ºå€åˆ†å“ªäº›æ˜¯ä¾†è‡ªæ–‡æª”çš„ä¿¡æ¯ï¼Œå“ªäº›æ˜¯ä½ çš„å°ˆæ¥­è£œå……ã€‚
5. å¦‚æœæ–‡æª”ä¸­å®Œå…¨æ²’æœ‰ç›¸é—œä¿¡æ¯ï¼Œè«‹èª å¯¦åœ°èªªæ˜ï¼Œä¸¦æä¾›ä½ çš„å°ˆæ¥­å»ºè­°æˆ–å¼•å°ç”¨æˆ¶å°‹æ‰¾æ›´å¤šè³‡æºã€‚
6. å›ç­”æ‡‰è©²å°ˆæ¥­ã€æº–ç¢ºã€æ˜“æ–¼ç†è§£ï¼Œä¸¦å¼•ç”¨ç›¸é—œçš„æ³•è¦æˆ–æ–‡æª”ä¾†æºã€‚
7. å°æ–¼æœƒè¨ˆã€ç¨…å‹™ç­‰å°ˆæ¥­å•é¡Œï¼Œè«‹æä¾›ç³»çµ±æ€§çš„å›ç­”ï¼Œè€Œä¸åƒ…åƒ…æ˜¯åˆ—å‡ºæ–‡æª”ä¸­æåˆ°çš„ç‰‡æ®µã€‚

æ–‡æª”å…§å®¹: {context}

è«‹è¨˜ä½ä»¥ä¸‹æ­¥é©Ÿä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼š
1. ä»”ç´°åˆ†æç”¨æˆ¶å•é¡Œçš„çœŸæ­£æ„åœ–å’Œèªç¾©
2. æ€è€ƒé€™å€‹å•é¡Œåœ¨è²¡å‹™ç¨…æ³•é ˜åŸŸçš„å°ˆæ¥­èƒŒæ™¯å’Œé‡è¦æ€§
3. å¾æä¾›çš„æ–‡æª”ä¸­æ‰¾å‡ºç›¸é—œä¿¡æ¯
4. çµ„ç¹”ä¸€å€‹çµæ§‹åŒ–ã€ç³»çµ±æ€§çš„å›ç­”ï¼Œè€Œä¸åƒ…åƒ…æ˜¯åˆ—å‡ºæ–‡æª”ç‰‡æ®µ
5. å¦‚æœ‰å¿…è¦ï¼Œè£œå……å°ˆæ¥­çŸ¥è­˜ä»¥æä¾›å®Œæ•´å›ç­”
6. ç¢ºä¿å›ç­”ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œå°ˆæ¥­æº–ç¢ºä¸”æ˜“æ–¼ç†è§£
"""

# åˆå§‹åŒ–å‘é‡å­˜å„²å’Œå°è©±éˆ
vectorstore = None
conversation_chain = None

# æ¨¡å‹å’Œåƒæ•¸è¨­ç½®
DEFAULT_MODEL = "gpt-3.5-turbo-16k"
DEFAULT_TEMPERATURE = 0.7
DEFAULT_TOP_K_SOURCES = 3
DEFAULT_SIMILARITY_THRESHOLD = 0.7

# ç”¨æˆ¶æœƒè©±è¨˜æ†¶å’Œè¨­ç½®
user_sessions = {}
user_settings = {}

def init_qa_chain():
    """åˆå§‹åŒ–å•ç­”éˆ"""
    global vectorstore, conversation_chain
    
    try:
        # æ§‹å»ºå‘é‡å­˜å„²çš„çµ•å°è·¯å¾‘
        vectorstore_path = os.path.join(PROJECT_ROOT, "data", "vectorstore")
        
        # æª¢æŸ¥å‘é‡å­˜å„²ç›®éŒ„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(vectorstore_path):
            logger.warning(f"å‘é‡å­˜å„²ç›®éŒ„ä¸å­˜åœ¨: {vectorstore_path}ï¼Œè«‹å…ˆé€šéStreamlitç•Œé¢æ›´æ–°çŸ¥è­˜åº«")
            return False
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = OpenAIEmbeddings()
        
        # åŠ è¼‰å‘é‡å­˜å„²
        import chromadb
        from chromadb.config import Settings
        
        client = chromadb.PersistentClient(
            path=vectorstore_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        collection_name = "finance_tax_documents"
        try:
            collections = client.list_collections()
        except Exception as e:
            logger.error(f"åˆ—å‡º ChromaDB é›†åˆæ™‚å‡ºéŒ¯ (å¯èƒ½æ˜¯ç‰ˆæœ¬ä¸ç›¸å®¹): {str(e)}")
            # å˜—è©¦åˆªé™¤ä¸¦é‡å»ºç©ºçš„ vectorstore
            try:
                import shutil
                shutil.rmtree(vectorstore_path)
                os.makedirs(vectorstore_path)
                logger.info(f"å·²åˆªé™¤ä¸¦é‡å»ºç©ºçš„ vectorstore ç›®éŒ„: {vectorstore_path}")
                client = chromadb.PersistentClient(
                    path=vectorstore_path,
                    settings=Settings(anonymized_telemetry=False)
                )
                collections = client.list_collections()
            except Exception as e2:
                logger.error(f"å˜—è©¦é‡å»º vectorstore å¤±æ•—: {str(e2)}")
                return True
        collection_exists = any(col.name == collection_name for col in collections)
        
        if not collection_exists:
            logger.warning(f"é›†åˆ {collection_name} ä¸å­˜åœ¨ï¼Œè«‹å…ˆé€šéStreamlitç•Œé¢æ›´æ–°çŸ¥è­˜åº«")
            return False
        
        # åŠ è¼‰å‘é‡å­˜å„²
        vectorstore = Chroma(
            client=client,
            collection_name=collection_name,
            embedding_function=embeddings,
            persist_directory=vectorstore_path
        )
        
        # å‰µå»ºæç¤ºæ¨¡æ¿
        system_message_prompt = SystemMessagePromptTemplate.from_template(SYSTEM_PROMPT)
        human_template = "{question}"
        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)
        
        # çµ„åˆæç¤ºæ¨¡æ¿
        chat_prompt = ChatPromptTemplate.from_messages([
            system_message_prompt,
            human_message_prompt
        ])
        
        # åˆå§‹åŒ–å°è©±éˆ
        conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(
                temperature=DEFAULT_TEMPERATURE,
                model=DEFAULT_MODEL
            ),
            combine_docs_chain_kwargs={"prompt": chat_prompt},
            retriever=vectorstore.as_retriever(
                search_kwargs={"k": DEFAULT_TOP_K_SOURCES * 3}  # æª¢ç´¢æ›´å¤šæ–‡æª”ç‰‡æ®µä»¥ç¢ºä¿æœ‰è¶³å¤ çš„ä¸é‡è¤‡ä¾†æº
            ),
            memory=None,  # æˆ‘å€‘å°‡åœ¨æ¯å€‹ç”¨æˆ¶æœƒè©±ä¸­å–®ç¨ç®¡ç†è¨˜æ†¶
            chain_type="stuff",
            verbose=True,
            return_source_documents=True
        )
        
        logger.info("å•ç­”éˆåˆå§‹åŒ–æˆåŠŸ")
        return True
    
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å•ç­”éˆæ™‚å‡ºéŒ¯: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False

# ç²å–ç”¨æˆ¶æœƒè©±
def get_user_session(platform, user_id):
    """ç²å–æˆ–å‰µå»ºç”¨æˆ¶æœƒè©±"""
    session_key = f"{platform}:{user_id}"
    if session_key not in user_sessions:
        user_sessions[session_key] = {
            "chat_history": [],
            "memory": ConversationBufferMemory(
                memory_key="chat_history",
                return_messages=True,
                output_key="answer"
            )
        }
    return user_sessions[session_key]

def get_user_settings(platform, user_id):
    """ç²å–ç”¨æˆ¶è¨­ç½®"""
    session_key = f"{platform}:{user_id}"
    if session_key not in user_settings:
        user_settings[session_key] = {
            "model": DEFAULT_MODEL,
            "temperature": DEFAULT_TEMPERATURE,
            "top_k_sources": DEFAULT_TOP_K_SOURCES,
            "similarity_threshold": DEFAULT_SIMILARITY_THRESHOLD
        }
    return user_settings[session_key]

def save_user_settings(platform, user_id, settings):
    """ä¿å­˜ç”¨æˆ¶è¨­ç½®"""
    session_key = f"{platform}:{user_id}"
    if session_key not in user_settings:
        user_settings[session_key] = {
            "model": DEFAULT_MODEL,
            "temperature": DEFAULT_TEMPERATURE,
            "top_k_sources": DEFAULT_TOP_K_SOURCES,
            "similarity_threshold": DEFAULT_SIMILARITY_THRESHOLD
        }
    
    # æ›´æ–°è¨­ç½®
    for key, value in settings.items():
        if key in ["model", "temperature", "top_k_sources", "similarity_threshold"]:
            user_settings[session_key][key] = value
    
    return user_settings[session_key]

# ä½¿ç”¨functools.lru_cacheè£é£¾å™¨ä¾†ç·©å­˜æŸ¥è©¢çµæœ
@functools.lru_cache(maxsize=50)
def cached_query(query, model, temperature, top_k, platform, user_id):
    """ç·©å­˜æŸ¥è©¢çµæœ"""
    if conversation_chain is None:
        return None
    
    # ç²å–ç”¨æˆ¶æœƒè©±
    session_key = f"{platform}:{user_id}"
    if session_key in user_sessions:
        chat_history = user_sessions[session_key]["chat_history"]
    else:
        chat_history = []
    
    # ä½¿ç”¨conversationéˆè™•ç†æŸ¥è©¢
    response = conversation_chain({
        "question": query,
        "chat_history": chat_history
    })
    
    return response

async def process_qa_query(platform: str, user_id: str, query: str, model: Optional[str] = None, temperature: Optional[float] = None, top_k_sources: Optional[int] = None, similarity_threshold: Optional[float] = None) -> Dict[str, Any]:
    """è™•ç†ç”¨æˆ¶æŸ¥è©¢ä¸¦è¿”å›çµæœ"""
    start_time = time.time()

    # åˆå§‹åŒ–å•ç­”éˆï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
    if conversation_chain is None:
        success = init_qa_chain()
        if not success:
            return {
                "response": "ç³»çµ±å°šæœªæº–å‚™å¥½ã€‚è«‹å…ˆé€šéStreamlitç•Œé¢æ›´æ–°çŸ¥è­˜åº«ï¼Œæˆ–è¯ç¹«ç®¡ç†å“¡ã€‚",
                "sources": [],
                "total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0,
                "total_cost": 0.0, "duration": time.time() - start_time
            }
    
    try:
        # ç²å–ç”¨æˆ¶æœƒè©±
        session = get_user_session(platform, user_id)
        
        # è¨­ç½®æ¨¡å‹åƒæ•¸
        model = model or DEFAULT_MODEL
        temperature = temperature or DEFAULT_TEMPERATURE
        top_k_sources = top_k_sources or DEFAULT_TOP_K_SOURCES
        similarity_threshold = similarity_threshold or DEFAULT_SIMILARITY_THRESHOLD
        
        # æ›´æ–°LLMåƒæ•¸
        conversation_chain.combine_docs_chain.llm_chain.llm = ChatOpenAI(
            temperature=temperature,
            model=model
        )
        
        # æ›´æ–°æª¢ç´¢å™¨åƒæ•¸
        conversation_chain.retriever.search_kwargs["k"] = top_k_sources * 3
        
        # ä½¿ç”¨get_openai_callbackä¾†è¿½è¹¤Tokenä½¿ç”¨æƒ…æ³
        with get_openai_callback() as cb:
            # è™•ç†æŸ¥è©¢
            response = cached_query(query, model, temperature, top_k_sources, platform, user_id)
            if response is None:
                response = conversation_chain({
                    "question": query,
                    "chat_history": session["chat_history"]
                })
            
            answer = response["answer"]
            source_documents = response.get("source_documents", [])
            
            # æå–ä¾†æº
            sources = []
            added_sources = set()
            added_doc_ids = set()
            added_content_hashes = set()
            unique_docs = []
            
            if source_documents:
                # æ‰‹å‹•éæ¿¾ç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼çš„æ–‡æª”
                try:
                    docs_with_scores = vectorstore.similarity_search_with_score(
                        query, k=len(source_documents) * 2
                    )
                    doc_scores = {doc.page_content[:50]: 1.0 - min(score, 1.0) for doc, score in docs_with_scores}
                    filtered_docs = [doc for doc in source_documents if doc_scores.get(doc.page_content[:50], 0) >= similarity_threshold]
                    source_documents = filtered_docs
                except Exception as e:
                    logger.warning(f"éæ¿¾ç›¸ä¼¼åº¦æ™‚å‡ºéŒ¯: {str(e)}ï¼Œé¡¯ç¤ºæ‰€æœ‰ä¾†æº")
                
                # å¢å¼·çš„å»é‡é‚è¼¯
                for doc in source_documents:
                    source = doc.metadata.get("source", "æœªçŸ¥ä¾†æº")
                    doc_id = doc.metadata.get("doc_id", "")
                    content_hash = hashlib.md5(doc.page_content[:200].encode()).hexdigest()
                    source_id = f"{source}"
                    if ((source_id not in added_sources) or (doc_id not in added_doc_ids and content_hash not in added_content_hashes)):
                        added_sources.add(source_id)
                        if doc_id: added_doc_ids.add(doc_id)
                        added_content_hashes.add(content_hash)
                        unique_docs.append(doc)
                
                displayed_sources = set()
                filtered_unique_docs = []
                for doc in unique_docs:
                    source = doc.metadata.get("source", "æœªçŸ¥ä¾†æº")
                    if source not in displayed_sources:
                        displayed_sources.add(source)
                        filtered_unique_docs.append(doc)
                        sources.append(source)
            
            # æ·»åŠ ä¾†æºä¿¡æ¯åˆ°å›ç­”
            if sources:
                sources_text = "\n\n**åƒè€ƒä¾†æºï¼š**\n"
                for i, source in enumerate(sources[:top_k_sources], 1):
                    doc_type = next((doc.metadata.get("type", "æ–‡ä»¶") for doc in filtered_unique_docs if doc.metadata.get("source") == source), "æ–‡ä»¶")
                    youtube_url = next((doc.metadata.get("youtube_url", "") for doc in filtered_unique_docs if doc.metadata.get("source") == source), "")
                    sources_text += f"{i}. {source} [YouTubeé€£çµ]({youtube_url})\n" if doc_type == "youtube" else f"{i}. {source}\n"
                answer_with_sources = f"{answer}\n{sources_text}"
            else:
                answer_with_sources = answer

            # è¨ˆç®—é‹è¡Œæ™‚é–“
            duration = time.time() - start_time

            # æ·»åŠ çµ±è¨ˆä¿¡æ¯
            stats_text = (
                f"\n\n---\n"
                f"ğŸ“Š **æœ¬æ¬¡æŸ¥è©¢è³‡è¨Š**\n"
                f"â±ï¸ **é‹è¡Œæ™‚é–“**: {duration:.2f} ç§’\n"
                f"ğŸ”¢ **ä½¿ç”¨Tokenæ•¸**: {cb.total_tokens} (æå•: {cb.prompt_tokens}, å›ç­”: {cb.completion_tokens})\n"
                f"ğŸ’° **é ä¼°èŠ±è²»**: ${cb.total_cost:.6f} USD"
            )
            final_response = f"{answer_with_sources}{stats_text}"

            # æ›´æ–°æœƒè©±æ­·å²
            session["chat_history"].append((query, answer))
            
            return {
                "response": final_response,
                "sources": sources[:3] if sources else [],
                "total_tokens": cb.total_tokens,
                "prompt_tokens": cb.prompt_tokens,
                "completion_tokens": cb.completion_tokens,
                "total_cost": cb.total_cost,
                "duration": duration
            }
    
    except Exception as e:
        logger.error(f"è™•ç†æŸ¥è©¢æ™‚å‡ºéŒ¯: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "response": f"è™•ç†æ‚¨çš„å•é¡Œæ™‚å‡ºéŒ¯: {str(e)}",
            "sources": [],
            "total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0,
            "total_cost": 0.0, "duration": time.time() - start_time
        }

# å•Ÿå‹•æ™‚åˆå§‹åŒ–å•ç­”éˆ
def startup_qa_client():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚çš„äº‹ä»¶è™•ç†"""
    logger.info("æ­£åœ¨å•Ÿå‹•è²¡å‹™ç¨…æ³•QAå®¢æˆ¶ç«¯...")
    init_qa_chain()

# åœ¨æ¨¡å¡Šè¼‰å…¥æ™‚åŸ·è¡Œåˆå§‹åŒ–
startup_qa_client()